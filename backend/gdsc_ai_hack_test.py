# -*- coding: utf-8 -*-
"""gdsc-ai-hack-test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V82OgJOKBeI1_oHK4Eq1da0RLWrT8KM9
"""

from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain.indexes import VectorstoreIndexCreator
from langchain_experimental.agents.agent_toolkits.csv.base import create_csv_agent
from langchain.agents.agent_types import AgentType
from langchain.memory import ConversationBufferMemory
import tiktoken
from langchain.text_splitter import CharacterTextSplitter

import os

# Prompt the user for their OpenAI API key
#api_key = input("Please enter your OpenAI API key: ")
api_key = "sk-proj-qZqgH1xD0nsvPHbATg4XT3BlbkFJHUH3p4syFuox0BlDYnWQ"

# Set the API key as an environment variable
os.environ["OPENAI_API_KEY"] = api_key

# Optionally, check that the environment variable was set correctly
print("OPENAI_API_KEY has been set!")
llm_model = "gpt-4-turbo"

# import a PDF file and convert it to text
import PyPDF2
def pdf_to_vectorstore(pdf_path):
    with open(pdf_file_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = ''
        for page_num in range(len(reader.pages)):
            text += reader.pages[page_num].extract_text()
    txt_file_path = f'/content/drive/MyDrive/{name}.txt'
    with open(txt_file_path, 'w', encoding='utf-8') as txt_file:
      txt_file.write(text)
    loader = TextLoader(file_path=txt_file_path, encoding="utf-8")
    data = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_text = text_splitter.split_documents(data)
    print(len(split_text[0].page_content))
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(split_text, embedding=embeddings)
    return vectorstore

def gen_chain(vectorstore):
    llm = ChatOpenAI(temperature=0.35, model_name="gpt-4-turbo")
    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vectorstore.as_retriever(),
        memory=memory
        )

name = "aos"
pdf_file_path = f"/content/drive/MyDrive/{name}.pdf"
vectorstore = pdf_to_vectorstore(pdf_file_path)
conversation_chain = gen_chain(vectorstore)

query = "Summarize the file into a mindmap, generating it in JSON format. Clearly separate different topics."
result = conversation_chain({"question": query})
answer = result["answer"]

map = answer[answer.index('{') : len(answer) - answer[::-1].index('}') ]
import json
def print_json_tree(data, indent=0):
    if isinstance(data, dict):
        for key, value in data.items():
            # if(indent!=0):
            print('    ' * indent + '└── ' + str(key))
            # else:
            # print(str(key))
            print_json_tree(value, indent + 1)
    elif isinstance(data, list):
        for item in data:
            print_json_tree(item, indent)
    else:
        # if(indent!=0):
        print('    ' * indent + '└── ' + str(data))
        # else:
            # print('    ' * indent + str(data))

# Carica il file JSON
data = json.loads(map)

# Stampa il JSON in formato ad albero
print_json_tree(data,0)

# per ciascuna entry della mappa io vorrei un bottone dove gli chiedi "What does this mean in this context? Provide details."
map

age = 20   ## PROFILE: discuss about it at the beginning of the conversation
query = f"Suggest a method for a {age}-year-old student to effectively learn what the file is about." # LASCO
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# OBJECTIVES
query = f"Create learning objectives to help a student effectively learn about this topic." # LASCO
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# LEARNING JOURNEY --> POSSIBILE ESTENSIONE FUTURA: integriamo testi che parlano di disturbi dell'apprendimento. Facciamo learning journeys tailor-made per gli studenti con difficoltà.
query = f"Propose a learning journey for a university student to learn about the topics in the document. Specify timeline information. Assume the student has little or no prior knowledge about the topics in the document."
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# EXERCISE GENERATION
context = "concurrency primitives in operating systems"
query = f"Test my knowledge about {context}. Propose some exercise, possibly."
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# ADDITIONAL RESOURCES
query = f"Provide extra resources, possibly searching the internet for them, to help a student learn about the topics of the document."
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# TAKE ONE STEP OF THE LEARNING JOURNEY AND GENERATE AN EXAMPLE EXERCISE
context = "Step 6 of the learning journey: Advanced Topics and Practical Implementation"
difficulty = "hard"
query = f"Test my knowledge about {context}. Propose some exercise with difficulty {difficulty}. Be very specific in the task description." # è vago, chiedi più approfonditamente se vuoi con un altro prompt --> metti i bottoni per chiedere un certo livello di dettaglio e delucidazioni
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# SOLUTION GENERATION --> "Is it correct? Does it make sense? Reason on it with the LLM. Possibly do it yourself and then compare." --> SPIRITO CRITICO
query = """
        Write a solution to the exercise you proposed before in C.
        Explain to me what are the takeaways.
        Stress what are the general principles I should learn doing the exercise.
        Cite sources where to find such principles."""
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# SUGGESTIONS AND RESOURCES FOR EXERCISES
query = """
        Point out resources useful for solving the exercise you proposed.
        Cite the sources from the Web and books."""
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# RAISE THE BAR
age = 20
context = "concurrency primitives in operating systems"
difficulty = "expert level"
query = f"Test my knowledge about {context}. Propose a {difficulty} excercise."
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# age = 20
# context = "concurrency primitives in operating systems"
# query = f"Referring to the advanced exercise you proposed before, show me a solutions thoroughly walking me through it. In particular, show me how to perform Throughput Optimization. Provide also some resources for me to extend my knowledge in such area."
# result = conversation_chain({"question": query})
# answer = result["answer"]
# print(answer)

age = 20
context = "concurrency primitives in operating systems"
query = f"Referring to the advanced exercise you proposed before, write a complete solution and thoroughly comment it. From now on only write C code."
result = conversation_chain({"question": query})
answer = result["answer"]
solution = answer[answer.index('{') : len(answer) - answer[::-1].index('}') ]
print(answer)

# identifica aree interessanti
# genera un piano di self-paced learning
# Fa quiz
# suggerisce un quiz al giorno
# TUNALO sulla difficoltà e sugli score
# da uno score --> non può valutarlo

# solution = answer[answer.index('{') : len(answer) - answer[::-1].index('}') ]
# query = f"This is my solution to the advanced exercise you assigned to me: {solution}. What do you think about it?" # GENERICO, NON VA BENE
# result = conversation_chain({"question": query})
# answer = result["answer"]
# print(answer)

#  GUIDE the user towards self-assessment.
query = f"This is my solution to the advanced exercise you assigned to me: {solution}. How can it be evaluated? What are the key aspects that should be considered? Cite sources and books if you can."
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# ASK about the errors
query = f"Based on my solution : {solution}, what are the most common pitfalls and mistakes I may make when dealing with similar problems in the future?" # non funziona benissimo
result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

# generate a wrong solution on purpose, tell the user to find the mistake, guide him towards it --> SPIRITO CRITICO
query = """
            Generate a wrong solution to the previous excercise you proposed.
            Introduce on purpose only one error.
            Explain the error you introduced.
            Provide recommendations on how to avoid doing it.
            Cite the sources from which you take recommendations.
        """

result = conversation_chain({"question": query})
answer = result["answer"]
print(answer)

"""lui non mi corregge, mi dice come correggermi citando i libri. Self-paced learning, spirito critico perchè mi correggo da solo."""